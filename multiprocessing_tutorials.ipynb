{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Borrowed from}$ \n",
    "\n",
    "$ \\textit{codebasics}$<br>\n",
    "https://www.youtube.com/playlist?list=PLeo1K3hjS3uub3PRhdoCTY8BxMKSW7RjN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snippet 1  <br>$\\textit{Hello World}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [i for i in range(5)]\n",
    "def getSquare(x):\n",
    "    for i in x:\n",
    "        time.sleep(2)\n",
    "        print('', end='s ')\n",
    "    print()\n",
    "    \n",
    "    \n",
    "def getCube(x):\n",
    "    for i in x:\n",
    "        time.sleep(3)\n",
    "        print('', end='c ')\n",
    "    print()\n",
    "    \n",
    "p1 = multiprocessing.Process(target = getSquare, args = (arr,))\n",
    "p2 = multiprocessing.Process(target = getCube, args = (arr,))\n",
    "\n",
    "p1.start()\n",
    "p2.start()\n",
    "\n",
    "\n",
    "p1.join()\n",
    "p2.join()\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snippet 2 <br>$\\textit{Shortcoming of global variables}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_result = []\n",
    "arr = [i for i in range(5)]\n",
    "def getSquare(x):\n",
    "    global square_result\n",
    "    for i in x:\n",
    "        square_result.append(i*i)\n",
    "    \n",
    "    \n",
    "    \n",
    "p1 = multiprocessing.Process(target = getSquare, args = (arr,))\n",
    "\n",
    "p1.start()\n",
    "p1.join()\n",
    "\n",
    "print(square_result)\n",
    "#Global variables are not shared amongst processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snippet 3 <br>$\\textit{Array}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Array as shared variable\n",
    "square_result = multiprocessing.Array('i',5) #Here i stands for integer. For double it is d\n",
    "arr = [i for i in range(5)]\n",
    "\n",
    "def getSquare(x, square_result):\n",
    "    for idx, item in enumerate(x):\n",
    "        square_result[idx] = item**2\n",
    "    \n",
    "    \n",
    "    \n",
    "p1 = multiprocessing.Process(target = getSquare, args = (arr, square_result))\n",
    "p1.start()\n",
    "p1.join()\n",
    "\n",
    "\n",
    "print(square_result[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snippet 4 <br>$\\textit{Value}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Value as shared Variable\n",
    "val = multiprocessing.Value('d',0.5)\n",
    "arr = [i for i in range(5)]\n",
    "\n",
    "def getSquare(x, val):\n",
    "    for item in x:\n",
    "        val.value+=(item**2)\n",
    "        print(val)\n",
    "    \n",
    "    \n",
    "    \n",
    "p1 = multiprocessing.Process(target = getSquare, args = (arr, val))\n",
    "p1.start()\n",
    "p1.join()\n",
    "\n",
    "\n",
    "print(val.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snippet 5 <br>$\\textit{Queue}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Queue as shared Variable\n",
    "# q = multiprocessing.Queue()\n",
    "arr = [i for i in range(5)]\n",
    "\n",
    "def getSquare(x, q):\n",
    "    for item in x:\n",
    "#         time.sleep(2)\n",
    "        q.put('s')\n",
    "    \n",
    "def getCube(x, q):\n",
    "    for item in x:\n",
    "#         time.sleep(3)\n",
    "        q.put('c')\n",
    "        \n",
    "#The time intervals have been chosen and are not random\n",
    "    \n",
    "p1 = multiprocessing.Process(target = getSquare, args = (arr, q))\n",
    "p2 = multiprocessing.Process(target = getCube, args = (arr, q))\n",
    "p1.start()\n",
    "p2.start()\n",
    "\n",
    "p1.join()\n",
    "p2.join()\n",
    "\n",
    "\n",
    "\n",
    "while q.empty() is False:\n",
    "    print(q.get(),end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snippet 6  <br>$\\textit{Locks}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without lock\n",
    "def deposit(balance):\n",
    "    for i in range(100):\n",
    "        time.sleep(0.01)\n",
    "        balance.value+=1\n",
    "        \n",
    "def withdraw(balance):\n",
    "    for i in range(100):\n",
    "        time.sleep(0.02)\n",
    "        balance.value-=1\n",
    "        \n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "    balance = multiprocessing.Value('i',200)\n",
    "    d = multiprocessing.Process(target = deposit, args = (balance,))\n",
    "    w = multiprocessing.Process(target = withdraw, args = (balance,))\n",
    "    d.start()\n",
    "    w.start()\n",
    "    d.join()\n",
    "    w.join()\n",
    "    \n",
    "    print(balance.value, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With lock\n",
    "def deposit(balance, lock):\n",
    "    for i in range(100):\n",
    "        time.sleep(0.01)\n",
    "        lock.acquire()\n",
    "        balance.value+=1\n",
    "        lock.release()\n",
    "        \n",
    "def withdraw(balance, lock):\n",
    "    for i in range(100):\n",
    "        time.sleep(0.01)\n",
    "        lock.acquire()\n",
    "        balance.value-=1\n",
    "        lock.release()\n",
    "        \n",
    "\n",
    "for i in range(20):\n",
    "    balance = multiprocessing.Value('i',200)\n",
    "    lock = multiprocessing.Lock()\n",
    "    d = multiprocessing.Process(target = deposit, args = (balance, lock))\n",
    "    w = multiprocessing.Process(target = withdraw, args = (balance, lock))\n",
    "    d.start()\n",
    "    w.start()\n",
    "    w.join()\n",
    "    d.join()\n",
    "    print(balance.value,end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snippet 6 <br>$\\textit{Divide among cores}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(n):\n",
    "    return n**2\n",
    "\n",
    "p = Pool()\n",
    "\n",
    "a = time.time()\n",
    "result = p.map(f, range(5)) #This alone will divide the work among all available cores equally\n",
    "p.close()\n",
    "p.join()\n",
    "b = time.time() - a\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(n):\n",
    "    sum = 0\n",
    "    for i in range(10000):\n",
    "        sum+=(i*i)\n",
    "    \n",
    "\n",
    "p = Pool()\n",
    "\n",
    "a = time.time()\n",
    "result = p.map(f, range(10000)) #This alone will divide the work among all available cores equally\n",
    "p.close()\n",
    "p.join()\n",
    "b = time.time() - a\n",
    "print('Time taken: {}'.format(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time.time()\n",
    "for i in range(10000):\n",
    "    sum = 0\n",
    "    for j in range(10000):\n",
    "        sum+=(j*j)\n",
    "\n",
    "b = time.time()- a\n",
    "print('Time taken: {}'.format(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mymodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mymodel,self).__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(3,2))\n",
    "        self.bias = nn.Parameter(torch.randn(2))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return torch.matmul(X,self.weight)+self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doTrain(a1,b1):    \n",
    "#     a1 = model()\n",
    "    optim = optimizer.Adam(lr=1e-3, params = a1.parameters())\n",
    "    optim.zero_grad()\n",
    "#     out = a1(X)\n",
    "    loss = b1.sum()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    \n",
    "    return list(a1.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 234, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/home/shouvik/Desktop/DeepLearning/dl/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 136, in reduce_tensor\n",
      "    raise RuntimeError(\"Cowardly refusing to serialize non-leaf tensor which requires_grad, \"\n",
      "RuntimeError: Cowardly refusing to serialize non-leaf tensor which requires_grad, since autograd does not support crossing process boundaries.  If you just want to transfer the data, call detach() on the tensor before serializing (e.g., putting it on the queue).\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 234, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/home/shouvik/Desktop/DeepLearning/dl/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 136, in reduce_tensor\n",
      "    raise RuntimeError(\"Cowardly refusing to serialize non-leaf tensor which requires_grad, \"\n",
      "RuntimeError: Cowardly refusing to serialize non-leaf tensor which requires_grad, since autograd does not support crossing process boundaries.  If you just want to transfer the data, call detach() on the tensor before serializing (e.g., putting it on the queue).\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 234, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/home/shouvik/Desktop/DeepLearning/dl/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 136, in reduce_tensor\n",
      "    raise RuntimeError(\"Cowardly refusing to serialize non-leaf tensor which requires_grad, \"\n",
      "RuntimeError: Cowardly refusing to serialize non-leaf tensor which requires_grad, since autograd does not support crossing process boundaries.  If you just want to transfer the data, call detach() on the tensor before serializing (e.g., putting it on the queue).\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 234, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/home/shouvik/Desktop/DeepLearning/dl/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 136, in reduce_tensor\n",
      "    raise RuntimeError(\"Cowardly refusing to serialize non-leaf tensor which requires_grad, \"\n",
      "RuntimeError: Cowardly refusing to serialize non-leaf tensor which requires_grad, since autograd does not support crossing process boundaries.  If you just want to transfer the data, call detach() on the tensor before serializing (e.g., putting it on the queue).\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 234, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/home/shouvik/Desktop/DeepLearning/dl/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 136, in reduce_tensor\n",
      "    raise RuntimeError(\"Cowardly refusing to serialize non-leaf tensor which requires_grad, \"\n",
      "RuntimeError: Cowardly refusing to serialize non-leaf tensor which requires_grad, since autograd does not support crossing process boundaries.  If you just want to transfer the data, call detach() on the tensor before serializing (e.g., putting it on the queue).\n"
     ]
    }
   ],
   "source": [
    "X = torch.ones(1,3, requires_grad = True)\n",
    "a1 = mymodel()\n",
    "b1 = a1(X)\n",
    "\n",
    "updatedParams = []\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    results = [executor.submit(doTrain, a1, b1) for _ in range(5)]\n",
    "    \n",
    "    for f in concurrent.futures.as_completed(results):\n",
    "        updatedParams.append(f.result())\n",
    "\n",
    "\n",
    "# with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "#     result = executor.map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Parameter containing:\n",
       "  tensor([[ 0.8683, -0.0678],\n",
       "          [ 1.2524, -0.7526],\n",
       "          [-0.5902,  0.4066]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-1.5302,  0.9537], requires_grad=True)],\n",
       " [Parameter containing:\n",
       "  tensor([[ 0.8683, -0.0678],\n",
       "          [ 1.2524, -0.7526],\n",
       "          [-0.5902,  0.4066]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-1.5302,  0.9537], requires_grad=True)],\n",
       " [Parameter containing:\n",
       "  tensor([[ 0.8683, -0.0678],\n",
       "          [ 1.2524, -0.7526],\n",
       "          [-0.5902,  0.4066]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-1.5302,  0.9537], requires_grad=True)],\n",
       " [Parameter containing:\n",
       "  tensor([[ 0.8683, -0.0678],\n",
       "          [ 1.2524, -0.7526],\n",
       "          [-0.5902,  0.4066]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-1.5302,  0.9537], requires_grad=True)],\n",
       " [Parameter containing:\n",
       "  tensor([[ 0.8683, -0.0678],\n",
       "          [ 1.2524, -0.7526],\n",
       "          [-0.5902,  0.4066]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-1.5302,  0.9537], requires_grad=True)]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updatedParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
